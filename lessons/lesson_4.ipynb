{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4\n",
    "## neural learning process - gradient descent\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep learning paradigm is about this -> \"predict, compare and learn\" (see lesson 3) <br>\n",
    "we saw the predict part of the paradigm on lesson 3. Onto \"compare\" now. <br>\n",
    "\n",
    "**Comparing gives a measurement of how much a prediction “missed” by* <br>\n",
    "\n",
    "In this step we'll learn about how to measure error, from a given value. There are many ways to calculate the measured error value. We'll learn **mean square error**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most basic mean squared error\n",
    "\n",
    "knob_weight = 0.5\n",
    "input = 0.5\n",
    "goal_pred = 0.8\n",
    "pred = input * knob_weight\n",
    "\n",
    "# why square? it forces the raw error to be positive, negative error doesnt make any sense, since\n",
    "# we're only here for the magnitude of the error.\n",
    "error = (pred - goal_pred) ** 2\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doesn’t squaring make big errors (>1) bigger and small errors (<1) smaller?\n",
    ">Yeah  ...  It’s  kind  of  a  weird  way  of  measuring  error,  but  it  turns  out  that  amplifying  big  errors and reducing small errors is OK. Later, you’ll use this error to help the network learn, and you’d rather it pay attention to the big errors and not worry so much about the small ones. Good parents are like this, too: they practically ignore errors if they’re small enough (breaking the lead on your pencil) but may go nuclear for big errors (crashing the car). See why squaring is valuable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot and Cold method of learning\n",
    "\n",
    "Hot and cold learning is simple. After making a prediction, you predict two more times, once with a slightly higher weight and again with a slightly lower weight. **You then move weight depending on which direction gave a smaller error**. Repeating this enough times eventually reduces error to 0. for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weight = 0.5\n",
    "input = 0.5\n",
    "\n",
    "goal_prediction = 0.8\n",
    "step = 0.002\n",
    "\n",
    "for i in range (1101):\n",
    "\tprediction = input * weight\n",
    "\terror = (prediction - goal_prediction)**2\n",
    "\n",
    "\tprint ('loop count: '+str(i+1)+' goal_prediction: 0.8 while '+'error: '+ str(error)+ \" prediction: \"+ str(prediction))\n",
    "\n",
    "\tup_pred = (weight+step) * input\n",
    "\tup_error = (up_pred - goal_prediction) ** 2\n",
    "\n",
    "\tdown_pred = (weight-step) * input\n",
    "\tdown_error = (down_pred - goal_prediction) ** 2\n",
    "\n",
    "\n",
    "\tif (down_error < up_error):\n",
    "\t\tweight = weight - step\n",
    "\n",
    "\tif (up_error < down_error):\n",
    "\t\tweight = weight + step\n",
    "        \n",
    "# issue 1: ovbiously inefficient\n",
    "# issue 2: need perfect step ammount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is GRADIENT DESCENT IN ACTION mothefucKA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## better method for hot and cold learning\n",
    "weight = 0.5\n",
    "input = 0.5\n",
    "goal_prediction = 0.8\n",
    "\n",
    "for i in range(20):\n",
    "    pred = input * weight \n",
    "    error = (goal_prediction - pred) ** 2\n",
    "    direction_and_amount = (pred - goal_prediction)*input\n",
    "    weight = weight - direction_and_amount\n",
    "#   print ('loop count: '+str(i+1)+' goal_prediction: 0.8 while '+'error: '+ str(error)+ \" prediction: \"+ str(prediction))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### direction and amount effect on last code\n",
    "---\n",
    "there are two parts in direction_and_amount\n",
    "* (pred - goal_pred): it is called pure error\n",
    "* input multiplied: does three things at one time:\n",
    "    * **stopping**: when the input is 0, there is no learning\n",
    "    * **negative reversal**: Ensures that weight moves in the correct direction even if input is negative\n",
    "    * **scaling**: Scaling is the third effect on the pure error caused by multiplying it by input. Logically, if input is big, your weight update should also be big. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating weights via a new parameter alpha, in gradeint descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 0.1\n",
    "# learning rate mothafucKA\n",
    "alpha = 0.01\n",
    "def neural_net (input, weights):\n",
    "    pred = input*weights\n",
    "    return pred\n",
    "\n",
    "ntoes = [8.5]\n",
    "win_or_lose = [1] #win\n",
    "\n",
    "input = ntoes[0]\n",
    "goal_pred = win_or_lose[0]\n",
    "\n",
    "pred = neural_net(input, weight)\n",
    "error = (pred - goal_pred) **2\n",
    "delta = pred - goal_pred #raw error\n",
    "\n",
    "weight_delta = delta * input\n",
    "weight -= weight_delta * alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following code is where the weight learns which way to go where error is reduced corretly to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.6400000000000001 Prediction:0.0 weight: 0.4\n",
      "Error:0.3600000000000001 Prediction:0.2 weight: 0.7000000000000001\n",
      "Error:0.2025 Prediction:0.35000000000000003 weight: 0.925\n",
      "Error:0.11390625000000001 Prediction:0.4625 weight: 1.09375\n",
      "Error:0.06407226562500003 Prediction:0.546875 weight: 1.2203125\n",
      "Error:0.036040649414062535 Prediction:0.61015625 weight: 1.315234375\n",
      "Error:0.020272865295410177 Prediction:0.6576171875 weight: 1.38642578125\n",
      "Error:0.011403486728668217 Prediction:0.693212890625 weight: 1.4398193359375\n",
      "Error:0.006414461284875877 Prediction:0.71990966796875 weight: 1.479864501953125\n",
      "Error:0.0036081344727426873 Prediction:0.7399322509765625 weight: 1.5098983764648437\n"
     ]
    }
   ],
   "source": [
    "weight, goal_pred, input = 0.0, 0.8, 0.5\n",
    "for i in range (10):\n",
    "    pred = input*weight\n",
    "    error = (pred - goal_pred) **2\n",
    "    delta = pred - goal_pred #raw error\n",
    "    weight_delta = delta * input\n",
    "    weight -= weight_delta\n",
    "    print(\"Error:\" + str(error) + \" Prediction:\" + str(pred)+ ' weight: '+ str(weight))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
